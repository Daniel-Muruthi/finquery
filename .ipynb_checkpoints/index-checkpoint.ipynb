{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba39d47-34fa-459c-8de2-67cedaa7487f",
   "metadata": {},
   "source": [
    "# **FinQuery: Intelligent Banking Intent Classification for Customer Support**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed80b6-8991-41ef-8cf8-cc6964022c48",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the rapidly evolving world of digital banking, customers expect instant, accurate responses to their queries—whether it’s checking a balance, reporting a lost card, or troubleshooting a payment issue. **FinQuery** is designed to meet this demand by automatically classifying customer support messages into one of 77 fine‑grained banking intents (the BANKING77 dataset). By leveraging a streamlined **TF‑IDF + Linear SVC** baseline alongside stronger baselines like **Logistic Regression** and **Multinomial Naive Bayes**, FinQuery demonstrates how even classic NLP pipelines can deliver high accuracy in complex, single‑domain intent detection. This notebook walks through each stage—from data exploration and preprocessing to model training, hyperparameter tuning, and final evaluation—showing how a disciplined approach to text normalization, stratified splits, and metric‑driven model selection yields a dependable automated routing solution for customer inquiries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f2713-49c0-4da4-92c2-f718fe12cbb0",
   "metadata": {},
   "source": [
    "# BUSINESS UNDERSTANDING\n",
    "\n",
    "- I aim to build a robust intent-classification model for BANKING77, which contains 77 fine-grained customer banking service intents.\n",
    "- This will help automated customer support route queries correctly, improving response time and customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd2939-8bdb-4dd8-8854-d3b5a4375d62",
   "metadata": {},
   "source": [
    "# DATA UNDERSTANDING\n",
    "\n",
    "- Train set: 10,003 examples\n",
    "- Test set:  3,080 examples\n",
    "- Number of Intents: 77\n",
    "- Data Source: https://huggingface.co/datasets/PolyAI/banking77"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3526b5-4da4-4ab4-9512-16e4b2c6167a",
   "metadata": {},
   "source": [
    "# PROJECT AIM\n",
    "\n",
    "- Accurately classify user queries into one of 77 intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb95e5b-708d-4166-b98a-66668776a3af",
   "metadata": {},
   "source": [
    "## My metrics of success are:\n",
    "\n",
    "**Accuracy** \n",
    "- Easy to understand; % of correct predictions out of all. Use for overall view.\n",
    "\n",
    "**Macro F1 Score**: \n",
    "- It balances both precision and recall\n",
    "- It doesn’t get skewed by class imbalance\n",
    "- It's meaningful when every intent matters, not just the frequent ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603e4a3-c18b-440c-9be3-234d09a47a2d",
   "metadata": {},
   "source": [
    "## **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5876ed5-e323-44a2-ba92-7ada08c88ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# For Bert Transformer\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609063b9-83bd-490e-863d-2055382d22f3",
   "metadata": {},
   "source": [
    "#### **Loading the Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb43ef-536b-4e54-9e9d-8cbb30ca69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataframe from the training Dataset\n",
    "# Load the training dataset\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b87fc-5f97-4d48-b73e-57b4dd3352ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataframe from the test Dataset\n",
    "# Load the test dataset\n",
    "\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300cd99e-eb61-4fce-93f5-cfef5649565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the Data\n",
    "\n",
    "print(f\"The Training Data Shape is: {train_df.shape}\")\n",
    "print(f\"The Test Data Shape is: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be1547-6851-4e52-907f-a67a31187cad",
   "metadata": {},
   "source": [
    "- Our Training data has 10,003 records and 2 columns\n",
    "- Our Test data has 3,080 records and 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c336b22-a222-40a0-aadc-8dae89012d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44222c9-7ce4-49b9-abf6-de030a4745dc",
   "metadata": {},
   "source": [
    "- The datatype of the data in the train_df both columns i.e., text and category is string i.e, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b09f4-2f03-4572-812b-a2e26aa808fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbac47a-431b-4aaa-bbbd-e8e3765eb958",
   "metadata": {},
   "source": [
    "- The datatype of the data in the test_df both columns i.e., text and category is string i.e, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fdaa44-cb1e-449d-977a-3dff4f5726e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the missing data in the training data\n",
    "\n",
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a0b75-4687-4c73-a461-4bbb75bd2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the missing data in the test data\n",
    "\n",
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca519b11-3f7f-4f00-978d-4838ad96304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out for duplicates in the training data\n",
    "\n",
    "len(train_df[train_df.duplicated(keep=\"first\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58d11c-7baa-4d7e-b47d-fa1b61f7e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out for duplicates in the test data \n",
    "\n",
    "len(test_df[test_df.duplicated(keep=\"first\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8f02a-46e7-4b79-883b-bbe927a969e6",
   "metadata": {},
   "source": [
    "- In both datasets there is neither missing data nor duplicates i.e., our data is pretty much clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5b572-2ec3-4882-929f-79c71886a7da",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b88147-469c-409c-9b6b-1f6835bfba41",
   "metadata": {},
   "source": [
    "### **Distribution of Intent category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66849e02-179c-4609-a7d7-67426ad16c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting top 10 distribution of intent category in the training data\n",
    "\n",
    "category_counts = train_df['category'].value_counts().sort_values(ascending=False).head(10)\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff85b75-2666-4fc8-8ff4-d27241118039",
   "metadata": {},
   "source": [
    "#### **Visualize category distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988e78c-d6ad-42d9-9635-7b1a3219564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent categories by frequency\n",
    "\n",
    "# Ensure data is sorted\n",
    "category_counts = category_counts.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Use seaborn barplot with the 'magma' palette\n",
    "# Pass the properly sorted index to the 'order' parameter\n",
    "ax = sns.barplot(\n",
    "    x=category_counts.values,\n",
    "    y=category_counts.index,\n",
    "    palette='magma',\n",
    "    order=category_counts.index  # This works because category_counts is now properly sorted\n",
    ")\n",
    "\n",
    "# No need to invert y-axis as the data is already in descending order and\n",
    "# seaborn plots from top to bottom by default\n",
    "\n",
    "# Add title and axis labels with styling\n",
    "ax.set_title(\"Top 10 Most Frequent Intents\", fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel(\"Number of Queries\", fontsize=14)\n",
    "ax.set_ylabel(\"\")  # no label on the y-axis\n",
    "\n",
    "# Improve the overall styling\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Increase tick label font size\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "# Annotate each bar with its count\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    ax.text(\n",
    "        width + width * 0.02,                      # x position: slightly past the end of the bar\n",
    "        p.get_y() + p.get_height() / 2,  # y position: center of the bar\n",
    "        f'{int(width):,}',                # text to display with comma formatting\n",
    "        va='center',\n",
    "        fontsize=12,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    \n",
    "# Add percentage\n",
    "total = category_counts.sum()\n",
    "for i, p in enumerate(ax.patches):\n",
    "    width = p.get_width()\n",
    "    percentage = (width / total) * 100\n",
    "    ax.text(\n",
    "        width + width * 0.12,                 \n",
    "        p.get_y() + p.get_height() / 2,  \n",
    "        f'({percentage:.1f}%)',           \n",
    "        va='center',\n",
    "        fontsize=11,\n",
    "        color='#666666'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8db12a-7778-42a6-b102-d7f121080b82",
   "metadata": {},
   "source": [
    "#### **WordCloud of full training text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a7b61-cea6-4e1c-8092-dac4bafaf487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Stop words to remove them from the word cloud\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "all_text = \" \".join(train_df['text'].tolist())\n",
    "wordcloud = WordCloud(\n",
    "    width=750,\n",
    "    height=450,\n",
    "    background_color='white',\n",
    "    max_words=200,\n",
    "    collocations=False,\n",
    "    stopwords=stop_words\n",
    ").generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud of Customer Queries\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ad855-6d2e-4845-8707-e1e7e4227078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CountVectorizer (Here I'll remove English stopwords)\n",
    "count_vect = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "\n",
    "# Fit my corpus\n",
    "X_counts = count_vect.fit_transform(train_df['text'])\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "word_counts = X_counts.sum(axis=0)  # returns a 1×V sparse matrix\n",
    "counts = [(word, word_counts[0, idx]) for word, idx in count_vect.vocabulary_.items()]\n",
    "\n",
    "# Sort by frequency and take top N\n",
    "top_n = 20\n",
    "top_words = sorted(counts, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "words, freqs = zip(*top_words)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_top = pd.DataFrame({\n",
    "    'word': words,\n",
    "    'frequency': freqs\n",
    "})\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_top, x='frequency', y='word', palette='magma')\n",
    "plt.title(\"Top 20 Most Common Words\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f1b56-a183-4cc8-917f-3751e91b62a0",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be63497-7e7c-4dc0-9bcc-e6996a14825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # remove any leading or trailing whitespace characters\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe0c5e5-1494-49c8-8ecd-25e9daf8b94c",
   "metadata": {},
   "source": [
    "I am keeping the preprocessing minimal—just lowercasing and whitespace normalization—for a few reasons:\n",
    "\n",
    "1. **TF‑IDF’s built‑in tokenization**  \n",
    "   Scikit‑learn’s `TfidfVectorizer` already:\n",
    "   * Splits on non‑alphanumeric boundaries,\n",
    "   * And builds the n‑gram vocabulary for you.  \n",
    "     Adding a separate tokenization step outside of the vectorizer would be redundant.\n",
    "\n",
    "2. **Stopword removal can hurt fine‑grained intents**  \n",
    "   In a 77‑way banking intent task, words like “to”, “on”, “did”, or “my” can carry important signals—for example:  \n",
    "   _“did my payment go through?”_ vs. _“why is my payment delayed?”_  \n",
    "   So we often keep stopwords in the bag‑of‑words representation or let the vectorizer handle them selectively.\n",
    "\n",
    "3. **Lemmatization adds extra complexity with limited gain**\n",
    "   * Lemmatization (via spaCy or NLTK) can reduce inflectional forms (e.g., “payments” → “payment”), but banking queries already tend to use a consistent vocabulary (“withdraw”, “withdrawal”; “pay”, “payment”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bf14b-a0eb-4b72-a21f-b23c30f8b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text columns\n",
    "\n",
    "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Sample cleaned text\n",
    "train_df[['text','clean_text']].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e44f1-f4d8-4def-989d-e4935c858e5a",
   "metadata": {},
   "source": [
    "#### **Train/Validation Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e74944-1265-4263-b24a-6cee4eb6f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Target vector (y) and Feature Matrix (X)\n",
    "\n",
    "X = train_df['clean_text']\n",
    "y = train_df['category']\n",
    "\n",
    "print(\"TF-IDF matrix feature shape:\", X.shape)\n",
    "print(\"Our Target Vector Shape is:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86283695-ceb1-4c96-b89e-8699f97add78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing train test split\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "#checking shapes of the both X_train and X_test sets\n",
    "print(\"X_train_raw:\", X_train_raw.shape, \"X_test_raw:\", X_test_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f295c-a535-4f17-b8e3-430b0a79eec4",
   "metadata": {},
   "source": [
    "#### **Feature Extraction (TF-IDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6babc0-89ec-4346-af39-3c23c02a99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vectorizer for text classification\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),  # Capture longer phrases that might be important\n",
    "    min_df=2,            # Removes terms that appear in fewer than 2 documents\n",
    "    max_df=0.95,         # Removes terms that appear in more than 95% of documents\n",
    "    sublinear_tf=True    # Reduces the weight of terms that occur very frequently in a document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563766d-bd84-4054-a6b0-4816ae29140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit vectorizer on training text\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train_raw)\n",
    "X_test = tfidf_vectorizer.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef03b83-aa85-42ef-9a34-3ffe915492ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape, \"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5981bc7-34b8-454e-80ab-35e48476c188",
   "metadata": {},
   "source": [
    "## **MODELLING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3789f17-b63a-4d3b-9484-b9d2758e2ddd",
   "metadata": {},
   "source": [
    "### **Linear SVC Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e28bc-8171-4644-8e82-c092d641fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training LinearSVC Model\n",
    "\n",
    "# Create Model\n",
    "svc_model = LinearSVC(class_weight='balanced', random_state=42, max_iter=5000)\n",
    "\n",
    "# Fit the Model\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "svc_prediction = svc_model.predict(X_test)\n",
    "\n",
    "# Test Accuracy, precision, recall, f1 score\n",
    "svc_accuracy = accuracy_score(y_test, svc_prediction)\n",
    "svc_precision = precision_score(y_test, svc_prediction, average='macro')\n",
    "svc_recall = recall_score(y_test, svc_prediction, average='macro')\n",
    "svc_f1_score = f1_score(y_test, svc_prediction, average='macro')\n",
    "\n",
    "print(f\"SVC Model Accuracy: {svc_accuracy:.4f}\")\n",
    "print(f\"SVC model Precision: {svc_precision:.4f}  (macro‐avg)\")\n",
    "print(f\"SVC Model Recall: {svc_recall:.4f}  (macro‐avg)\")\n",
    "print(f\"SVC F1 Score: {svc_f1_score:.4f}  (macro‐avg)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0f008-9dce-4ee4-9abc-8f39455c0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report - LinearSVC\")\n",
    "print(classification_report(y_test, svc_prediction, zero_division=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48060458-911c-422a-8732-166e17b298a1",
   "metadata": {},
   "source": [
    "### **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6d546-cc52-4218-9b08-d9f86d7805ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Logistic Regression Model\n",
    "\n",
    "# Create model\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight=\"balanced\")\n",
    "\n",
    "# Fit model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "lr_prediction = lr_model.predict(X_test)\n",
    "\n",
    "# Test Accuracy, precision, recall, f1 score\n",
    "lr_accuracy = accuracy_score(y_test, lr_prediction)\n",
    "lr_precision = precision_score(y_test, lr_prediction, average='macro')\n",
    "lr_recall = recall_score(y_test, lr_prediction, average='macro')\n",
    "lr_f1_score = f1_score(y_test, lr_prediction, average='macro')\n",
    "\n",
    "print(f\"Logistic Regression Model Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression model Precision: {lr_precision:.4f} (macro‐avg)\")\n",
    "print(f\"Logistic Regression Model Recall: {lr_recall:.4f} (macro‐avg)\")\n",
    "print(f\"Logistic Regression F1 Score: {lr_f1_score:.4f} (macro‐avg)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca3ee2-387d-408a-b9ed-1ace1dd381ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report - Logistic Regression\")\n",
    "print(classification_report(y_test, lr_prediction, zero_division=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2d22c-361f-437b-b1b1-5916107e5179",
   "metadata": {},
   "source": [
    "### **Naive Bayes Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62699bd5-a6ee-40c0-a20f-691fec5e5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Naive Bayes model\n",
    "\n",
    "# Create Model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Fit Model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "\n",
    "# Test Accuracy, precision, recall, f1 score\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "nb_precision = precision_score(y_test, nb_predictions, average='macro')\n",
    "nb_recall = recall_score(y_test, nb_predictions, average='macro')\n",
    "nb_f1_score = f1_score(y_test, nb_predictions, average='macro')\n",
    "\n",
    "# Evaluation results\n",
    "print(f\"Naive Bayes Accuracy:         {nb_accuracy:.4f}\")\n",
    "print(f\"Naive Bayes Precision:        {nb_precision:.4f}  (macro avg)\")\n",
    "print(f\"Naive Bayes Recall:           {nb_recall:.4f}  (macro avg)\")\n",
    "print(f\"Naive Bayes F1 Score:         {nb_f1_score:.4f}  (macro avg)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59305082-bc83-4d38-8981-d3cff6ff6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, nb_predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45904a22-424e-473d-a373-f829a9f66703",
   "metadata": {},
   "source": [
    "#### **Getting Best Performing Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef865e-7a0e-4e00-9e8b-c1b0835fc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Logistic Regression', 'Linear SVC', 'Naive Bayes']\n",
    "accuracy = [lr_accuracy, svc_accuracy, nb_accuracy]\n",
    "f1 = [lr_f1_score, svc_f1_score, nb_f1_score]\n",
    "precision = [lr_precision, svc_precision, nb_precision]\n",
    "recall = [lr_recall, svc_recall, nb_recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533d56a-16e5-49b2-985f-a8a780a4497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracy,\n",
    "    'Macro F1': f1,\n",
    "    'Macro Precision': precision,\n",
    "    'Macro Recall': recall\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33fd2d-a1b0-4376-8abb-c3f430a3f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt for plotting\n",
    "df_melted = metrics_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_melted, x='Metric', y='Score', hue='Model')\n",
    "plt.title('Model Comparison')\n",
    "plt.ylim(0.75, 0.90)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f2095-d33f-445b-b9f2-3f8dfb501e41",
   "metadata": {},
   "source": [
    "- The best performing model from the barplot above is definitely the Linear SVC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f3352-37b0-4f3b-97b9-f296d9ed460b",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning for Best Model (i.e., LinearSVC)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ad335-7ce4-4e1a-bdab-d6ceb0e74f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "# Define Hyperparameter Grid\n",
    "param_grid = {\n",
    "    'svc__class_weight': ['balanced'],\n",
    "    'svc__random_state': [42],\n",
    "    'svc__max_iter': [1000, 2000, 5000],\n",
    "    'svc__C': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Grid Search with 5-fold CV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit your data\n",
    "grid_search.fit(X_train_raw, y_train)\n",
    "\n",
    "# Print best model details\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Macro F1 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0824e4-12a0-417f-844d-2a15d5802dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "\n",
    "tuned_prediction = grid_search.predict(X_test_raw)\n",
    "print(classification_report(y_test, tuned_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253f0bf-190f-49a7-bb78-9d6fd4f1c671",
   "metadata": {},
   "source": [
    "### **Saving The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced94d02-fdc3-4423-aab5-1b2c106b9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire GridSearchCV object\n",
    "\n",
    "joblib.dump(grid_search, 'tuned_linear_svc_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b914ed-a931-4e79-910c-87bbc0c56fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF Vectorizer\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf29b4-0f80-4649-a93c-b0cc77e773a8",
   "metadata": {},
   "source": [
    "### **TESTING MODEL ON UNSEEN DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d9112-45bf-44d0-a251-7aff36c61835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test data feature matrix\n",
    "\n",
    "X_real_test = test_df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f424275-c391-4fb8-a774-89a9a2be6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels\n",
    "\n",
    "y_real_test = test_df['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4587d4-9f86-45ef-a99a-26c02b53a4a5",
   "metadata": {},
   "source": [
    "#### **Load Model and Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb342e-a247-4f09-addf-2a31421596a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my trained model\n",
    "\n",
    "loaded_model = joblib.load('tuned_linear_svc_model.pkl')\n",
    "\n",
    "# predict on the unseen data\n",
    "\n",
    "y_real_pred = loaded_model.predict(X_real_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9c7de-197a-42f5-8a0c-0ca95d7330de",
   "metadata": {},
   "source": [
    "#### **Evaluate Model Performance on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45805dcd-517b-406a-8aaf-a8caa9214d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the true test set\n",
    "real_test_accuracy = accuracy_score(y_real_test, y_real_pred)\n",
    "real_test_macro_f1 = f1_score(y_real_test, y_real_pred, average='macro')\n",
    "\n",
    "print(\"Final Evaluation on Real Test Set:\")\n",
    "print(f\"Accuracy: {real_test_accuracy:.4f}\")\n",
    "print(f\"Macro F1 Score: {real_test_macro_f1:.4f} (macro‐avg)\")\n",
    "print(\"\\nFull Classification Report:\")\n",
    "print(classification_report(y_real_test, y_real_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4a523-5761-4366-a460-0324194781c5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. **Performance Recap**  \n",
    "   - Our best performing model, **Linear SVC** (with class‑weight balancing and tuned C parameter), achieved a **macro‑F1** of **≈0.87** on the unseen test set, ensuring robust handling across all 77 intent categories.  \n",
    "   - The **Logistic Regression** and **Naive Bayes** baselines also showed competitive macro‑F1 scores of **≈0.84** and **≈0.77**, highlighting the strength of even simple TF‑IDF pipelines.\n",
    "\n",
    "2. **Key Takeaways**  \n",
    "   - **Minimal preprocessing** (lowercasing, whitespace normalization) paired with **TF‑IDF n‑grams** is sufficient to capture the nuance in banking queries.  \n",
    "   - **Stratified sampling** and **macro‑averaged metrics** are essential for fair evaluation when classes are imbalanced.  \n",
    "   - **Hyperparameter tuning** (via `GridSearchCV`) can yield noticeable gains—here, a ~1% lift in macro‑F1 for Linear SVC.\n",
    "\n",
    "3. **Next Steps**  \n",
    "   - **Transformer‑based fine‑tuning** (e.g., BERT or FinBERT) to capture deeper semantic patterns and further improve low‑resource intents.  \n",
    "   - **Active learning** to continuously incorporate new customer queries and emerging intents into the training set.  \n",
    "   - **Production deployment** via a lightweight FastAPI service, with a confidence‑based fallback to human agents for low‑certainty inputs.\n",
    "\n",
    "FinQuery’s pipeline shows that with careful preprocessing, rigorous validation, and thoughtful metric selection, automated intent classification can be both **accurate** and **interpretable**, ready to enhance your bank’s customer support with near‑real‑time routing and resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30bedfc-3221-4efd-9eb4-4ed94fecb85a",
   "metadata": {},
   "source": [
    "## **Fine-Tune BERT for Intent Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebffde1-a263-4085-bbb8-1763b8c32689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Labels\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_df[\"label\"] = le.fit_transform(train_df[\"category\"])\n",
    "test_df[\"label\"]  = le.transform(test_df[\"category\"])\n",
    "num_labels = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da04263-83f9-422e-b145-25151b37329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PyTorch Dataset\n",
    "\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text  = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids':      encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels':         torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff50d5-588d-4e3a-b65e-1ed6fe259115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Tokenizer & Datasets\n",
    "\n",
    "tokenizer     = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = IntentDataset(\n",
    "    texts=train_df[\"text\"].tolist(),\n",
    "    labels=train_df[\"label\"].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128\n",
    ")\n",
    "eval_dataset  = IntentDataset(\n",
    "    texts=test_df[\"text\"].tolist(),\n",
    "    labels=test_df[\"label\"].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b66e0b-1df0-4501-b65d-0ede1b6475e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained BERT for Sequence Classification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a08d7-3c50-47eb-994a-a2044a7c9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments & Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_finetuned',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels      = pred.label_ids\n",
    "    preds       = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = \\\n",
    "        __import__('sklearn.metrics').metrics.precision_recall_fscore_support(\n",
    "            labels, preds, average='macro', zero_division=0\n",
    "        )\n",
    "    acc = __import__('sklearn.metrics').metrics.accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision_macro': precision,\n",
    "        'recall_macro': recall,\n",
    "        'f1_macro': f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5af75d-663d-465b-bd64-769c6fadceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Evaluation\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca2ddee-d771-4063-a86a-783acec2175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving  Model & Tokenizer & Label Encoder\n",
    "\n",
    "model.save_pretrained(\"./bert_intent_model\")\n",
    "\n",
    "tokenizer.save_pretrained(\"./bert_intent_model\")\n",
    "\n",
    "joblib.dump(le, \"intent_label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520f9b9-4b39-431e-897c-af771a634180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function for Inference\n",
    "\n",
    "def predict_intent(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the predicted intent label for a single banking query.\n",
    "    \"\"\"\n",
    "    # 1) clean text\n",
    "    cleaned = clean_text(text)\n",
    "    # 2) tokenize\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        cleaned,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # 3) model forward + softmax\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask']\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    pred_id = logits.argmax(dim=-1).item()\n",
    "    return le.inverse_transform([pred_id])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
